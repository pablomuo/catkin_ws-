#!/usr/bin/env python
#################################################################################
#Copyright 2022 Elizabeth
#
#Licensed under the Apache License, Version 2.0 (the "License");
#you may not use this file except in compliance with the License.
#You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
#Unless required by applicable law or agreed to in writing, software
#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#distributed under the License is distributed on an "AS IS" BASIS,
#See the License for the specific language governing permissions and
#limitations under the License.
#################################################################################
import rospy
import os
# os.environ["CUDA_VISIBLE_DEVICES"]="-1"
import json
import numpy as np
import random
import time
import pickle

import tensorflow as tf

import sys
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))
from environment_v1 import Behaviour
from collections import deque
from std_msgs.msg import Float32MultiArray
from robot_v1 import Robot
# from tensorflow.keras.models import Sequential, load_model
# from tensorflow.keras.optimizers import Adam, RMSprop
# from tensorflow.keras.layers import Dense, Dropout, Activation
# from tensorflow.keras.callbacks import History, TerminateOnNaN, EarlyStopping, ReduceLROnPlateau
from keras.models import Sequential, load_model
from keras.optimizers import Adam,RMSprop
from keras.layers import Dense, Dropout, Activation
from keras.callbacks import History, TerminateOnNaN, EarlyStopping, ReduceLROnPlateau
import keras
import tensorflow as tf

gpu_options = tf.GPUOptions(allow_growth=True)
sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
keras.backend.tensorflow_backend.set_session(sess)

from numba import cuda
tf.logging.set_verbosity(tf.logging.ERROR)

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

class ReinforcementNetwork(object):
    """
    Algorithm DDRLE-GE
    """
    def __init__(self,state_size,action_size,episodes):

        self.dirPath            = os.path.dirname(os.path.realpath(__file__))
        self.dirPath            = self.dirPath.replace('ddrl_ge/src/nodes', 'ddrl_ge/src/save_model/environment_')
        self.state_size         = state_size
        self.action_size        = action_size
        self.episodes           = episodes
        self.episode_step       = 6000
        self.target_update      = 2000
        self.discount_factor    = 0.99
        self.learning_rate      = 0.00030 #0.00025
        self.batch_size         = 96#128s
        self.train_start        = 96#128
        self.Pa                 = 0
        self.Pbest              = 0.01
        self.Pbest_max          = 0.95
        # self.increase_factor    = 0.98#0.975
        self.increase_factor    = 0.97#70.975 env1 for paper
        self.size_layer_1       = 512#1000
        self.size_layer_2       = 512#512 256
        self.size_layer_3       = 256#64
        self.size_layer_4       = 20#64
        self.reward_max         = 0
        self.tau                = 0.1
        self.target_value       = 0
        self.dropout            = 0.2
        self.lim_q_s            = 0.95
        self.lim_q_i            = 0.25
        self.start_or           = 0.186
        self.lim_train          = 0.186#0.25#0.36
        self.memory_D           = deque(maxlen=100000)
        self.memory_GT          = deque(maxlen=100000)
        self.memory_EPS         = deque(maxlen=100000)
        self.load_model         = True
        self.load_episode       = 1020#665
        self.loss               = 'mse'
        self.activation_output  = 'linear'
        self.activation_layer   = 'relu'
        self.kernel_initializador = 'lecun_uniform'
        self.q_model              = self.q_network()
        self.target_model         = self.target_network()
        self.tf_rewards           = 0
        # self.update_target_network()

        if self.load_model:
            self.q_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_q_model'+".h5").get_weights())
            self.target_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_target_model'+".h5").get_weights())
            self.Pa,self.Pbest= self.load_mode()
        else:
            self.q_model              = self.q_network()
            self.target_model         = self.target_network()

    def load_mode(self):
        # self.q_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_target_model'+".h5").get_weights())
        # self.target_model.set_weights(load_model(self.dirPath+str(self.load_episode)+'_target_model'+".h5").get_weights())
        # q_model=load_model(self.dirPath+str(self.load_episode)+'_q_model'+".h5")
        # target_model=load_model(self.dirPath+str(self.load_episode)+'_target_model'+".h5")
        with open(self.dirPath+str(self.load_episode)+'.json') as outfile:
            param = json.load(outfile)
            Pa = param.get('Pa')
            Pbest = param.get('Pbest')
        return  Pa, Pbest
        # return q_model,target_model, Pa, Pbest

    def q_network(self):
        '''
        In this network we evaluate the action of the q_network and predict the following value of Q(s',a)
        '''
        q_model = Sequential()
        q_model.add(Dense(self.size_layer_1, input_shape=(self.state_size,), activation= self.activation_layer, kernel_initializer = self.kernel_initializador))
        q_model.add(Dense(self.size_layer_2, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        # q_model.add(Dense(self.size_layer_3, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        # q_model.add(Dense(self.size_layer_4, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        q_model.add(Dropout(self.dropout))
        q_model.add(Dense(self.action_size, kernel_initializer=self.kernel_initializador))
        q_model.add(Activation(self.activation_output))
        # q_model.compile(loss=self.loss, optimizer=Adam(lr=self.learning_rate), metrics=['acc'])
        # self.tf_rewards=tf.placeholder(tf.float64,shape=[None])
        # los=tf.reduce_sum(self.tf_rewards*self.loss)

        q_model.compile(loss=self.loss, optimizer=RMSprop(lr=self.learning_rate, rho=0.9,  epsilon=1e-08, decay=0.0), metrics=['acc'])
        return q_model

    def target_network(self):
        '''
        In this network we evaluate the action of the q_network and predict the following value of Q(s',a)
        '''
        target_model = Sequential()
        target_model.add(Dense(self.size_layer_1, input_shape=(self.state_size,), activation= self.activation_layer, kernel_initializer = self.kernel_initializador))
        target_model.add(Dense(self.size_layer_2, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        # target_model.add(Dense(self.size_layer_3, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        # target_model.add(Dense(self.size_layer_4, activation= self.activation_layer, kernel_initializer=self.kernel_initializador))
        target_model.add(Dropout(self.dropout))
        target_model.add(Dense(self.action_size, kernel_initializer=self.kernel_initializador))
        target_model.add(Activation(self.activation_output))
        # target_model.compile(loss=self.loss, optimizer=Adam(lr=self.learning_rate), metrics=['acc'])
        # los=tf.reduce_sum(self.tf_rewards*self.loss)
        target_model.compile(loss=self.loss, optimizer=RMSprop(lr=self.learning_rate, rho=0.9, epsilon=1e-08, decay=0.0), metrics=['acc'])
        return target_model

    def get_Qvalue(self, reward, next_target, done):
        if done:
            return reward
        else:
            return reward + self.discount_factor * next_target
    def update_target_network(self):
        q_model_theta = self.q_model.get_weights()
        target_model_theta = self.target_model.get_weights()
        counter = 0
        for q_weight, target_weight in zip(q_model_theta, target_model_theta):
            target_weight = target_weight *(1-self.tau) + q_weight*self.tau
            target_model_theta[counter] = target_weight
            counter += 1
        rospy.loginfo("UPDATE TARGET NETWORK")
        return self.target_model.set_weights(target_model_theta)

    def get_Pa(self):
        '''
        Calculates the probability by "Semi-Uniform Distributed Exploration"
        Pbes=0 purely random exploration, Pbest=1 pure exploitation.
        '''
        self.Pa = self.Pbest + ((1.0-self.Pbest)/self.action_size)
        # print(self.Pa)
        return self.Pa

    def get_action(self,state):
        '''
        Action is determined based on directed knowledge, hybrid knowledge
        or autonomous knowledge
        '''
        n2 = np.random.rand()
        n3 = np.random.rand()

        if self.Pa <= self.lim_q_i:
            self.q_value = np.zeros(self.action_size)
            action = None
            evolve_rule = True
            print("rules1")

        elif self.lim_q_s>self.Pa >self.lim_q_i:
            if n3 > self.Pa:
                self.q_value = np.zeros(self.action_size)
                action = None
                evolve_rule = True
                print("rules2")

            else:
                # self.q_value = self.q_model.predict(state.reshape(1,len(state)))
                # q_value = self.q_value[0][:-1]
                # mask=~(np.arange(self.action_size -1)==np.argmax(q_value))
                # q_value=q_value[mask]
                # q_value= q_value-min(q_value)
                # q_value = q_value/sum(q_value)
                # action = np.random.choice(np.arange(self.action_size-1)[mask],p=q_value)
                # evolve_rule = False
                # print("network-rules")
                #
                self.q_value = self.q_model.predict(state.reshape(1,len(state)))
                action = np.argmax(self.q_value[0][:5])
                evolve_rule = False
                print("1 Best action-rule")

        else:
            if n2 <= self.Pa:
                self.q_value = self.q_model.predict(state.reshape(1,len(state)))
                action = np.argmax(self.q_value[0][:5])
                evolve_rule = False
                print("1 Best action")
                # print("evolve_rul 1",evolve_rule)

            else:
                self.q_value = self.q_model.predict(state.reshape(1, len(state)))
                q_value = self.q_value[0][:-1]
                mask=~(np.arange(self.action_size -1)==np.argmax(q_value))
                q_value=q_value[mask]
                q_value= q_value-min(q_value)
                q_value = q_value/sum(q_value)
                action = np.random.choice(np.arange(self.action_size-1)[mask],p=q_value)
                evolve_rule = False
                print("2 Best action")

        return action, evolve_rule

    def append_D(self, state, action, reward, next_state, done):
        '''
        Memory used to train the model
        '''
        self.memory_D.append((state, action, reward, next_state, done))

    def append_EPS(self, state,action, reward, next_state, done):
        '''
        Memory for each episode,
        '''
        self.memory_EPS.append((state,action, reward, next_state, done))

    def winning_state(self):
        '''
        When the robot reaches the target, the temporary memory is copied into the
        main memory depending on the average reward.
        '''
        all_rewards = map(lambda x: x[2],self.memory_EPS)
        reward_aver = np.mean(all_rewards)
        if reward_aver > self.reward_max:
            self.reward_max = reward_aver
            self.memory_D.extend(self.memory_EPS)
            self.memory_D.extend(self.memory_EPS)
            self.memory_D.extend(self.memory_EPS)
            self.memory_EPS.clear()
            rospy.loginfo("Winning State with reward_max !!!")
        else:
            self.memory_EPS.clear()
            rospy.loginfo("Normal Win !!!")

    def best_state(self):
        '''
        When the robot reaches the goal with the best time, the temporary
        memory is copied into the main memory depending on the best time.
        '''
        self.memory_GT.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_D.extend(self.memory_EPS)
        self.memory_EPS.clear()
        rospy.loginfo("Great Time !!!")

    def experience_replay(self):
        '''
        Based on probability choose random samples or continuous samples with
        the best average rewards
        '''
        batch_save   = []
        max_rew_save = []
        for i in range(1):
            num_2 = random.randrange(0,len(self.memory_D)-int(self.batch_size/8.0))
            if len(self.memory_GT)>2:
                mini_batch1 = deque(np.array(self.memory_D)[num_2:num_2+int(self.batch_size/8.0)-2])
            else:
                mini_batch1 = deque(np.array(self.memory_D)[num_2:num_2+int(self.batch_size/8.0)])
            batch_save.append(mini_batch1)
            all_rewards = np.array(map(lambda x: x[2],mini_batch1))
            max_reward = np.sum(all_rewards)
            max_rew_save.append(max_reward)
        idx_max_ = np.argmax(max_rew_save)

        if len(self.memory_GT)>2:
            id_gt =random.sample(self.memory_GT, 2)
            id_ra =random.sample(self.memory_D,int(self.batch_size-(self.batch_size/8.0)))
            mini_batch = batch_save[idx_max_]
            mini_batch.extend(id_gt)
            mini_batch.extend(id_ra)
        else:
            id_ra =random.sample(self.memory_D,int(self.batch_size-(self.batch_size/8.0)))
            mini_batch = batch_save[idx_max_]
            mini_batch.extend(id_ra)
        return mini_batch

    def train_model(self):
        mini_batch =  self.experience_replay()
        X_batch = np.empty((0, self.state_size), dtype=np.float64)
        Y_batch = np.empty((0, self.action_size), dtype=np.float64)

        for i in range(self.batch_size):
            states = mini_batch[i][0]
            actions = mini_batch[i][1]
            rewards = mini_batch[i][2]
            next_states = mini_batch[i][3]
            dones = mini_batch[i][4]
            # print(states)
            # print(len(states))
            q_value = self.q_model.predict(states.reshape(1, len(states)))
            self.q_value = q_value
            next_q_value = self.q_model.predict(next_states.reshape(1, len(next_states)))
            id_max_1 = np.argmax(next_q_value)

            # if target:
            next_target = self.target_model.predict(next_states.reshape(1, len(next_states)))
            # else:
                # next_target = self.q_model.predict(next_states.reshape(1, len(next_states)))
            next_t =next_target[0][id_max_1]
            self.target_value = next_t
            next_q_value = self.get_Qvalue(rewards, next_t, dones)

            X_batch = np.append(X_batch, np.array([states.copy()]), axis=0)
            Y_sample = q_value.copy()

            Y_sample[0][actions] = next_q_value
            Y_batch = np.append(Y_batch, np.array([Y_sample[0]]), axis=0)

            if dones:
                X_batch = np.append(X_batch, np.array([next_states.copy()]), axis=0)
                Y_batch = np.append(Y_batch, np.array([[rewards] * self.action_size]), axis=0)
        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=0, min_lr=0.0001)
        # earlystop_callback = EarlyStopping(monitor='val_loss', mode='min', min_delta=0.0001,patience=50)
        # print(X_batch)
        result = self.q_model.fit(X_batch, Y_batch, batch_size=self.batch_size, epochs=1, verbose=0,callbacks=[reduce_lr])
        history_dict = result.history
        acc  = result.history['acc'][0]
        loss = (result.history['loss'][0])*self.tf_rewards
        lr =  result.history['lr'][0]
        if not os.path.exists(agent.dirPath +"loss_history"+'.txt'):
            with open(agent.dirPath +"loss_history"+'.txt', 'a') as outfile:
                outfile.write("acc".rjust(15," ")+ "   "+"loss".rjust(20," ")+ "   "+"lr".rjust(10," ") +"\n")
        with open(agent.dirPath +"loss_history"+ '.txt', 'a') as outfile:
            outfile.write("{: .3e}".format(acc)+"   "+"{: .3e}".format(loss)+ "   "+"{: .3e}".format(lr) +"   "+"\n")

if __name__ == '__main__':
        # gpu =tf.config.list_physical_devices('GPU')
        # if gpu:
        #     tf.config.set_visible_devices(gpu[0],'GPU')
    #
    # tf.compat.v1.disable_eager_execution()
    #
    # gpus = tf.config.experimental.list_physical_devices('GPU')
    # for device in gpus:
    #     tf.config.experimental.set_memory_growth(device, True)
    # # # cpus = tf.config.experimental.list_physical_devices('CPU')
    # if gpus:
    #   # Restrict TensorFlow to only use the first GPU
    #   try:
    #     tf.config.experimental.set_visible_devices(gpus[0], 'GPU')
    #     logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    #     print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPU")
    #   except RuntimeError as e:
    #   # Visible devices must be set before GPUs have been initialized
    #     print(e)
    rospy.init_node('network_v1')
    # rate = rospy.Rate(10)
    state_size   = 28
    action_size  = 6
    episodes     = 19000
    ep           = 0
    global_step  = 0

    env          = Behaviour()
    robot        = Robot(action_size,env)
    agent        = ReinforcementNetwork(state_size, action_size,episodes)
    start_time   = time.time()
    agent.get_Pa()
    #with tf.device('/GPU:0'):
    for e in range(agent.load_episode +1, episodes):

        ep += 1
        done = False
        finish = False
        robot.reset()
        env.reset_gazebo()
        time.sleep(0.5)
        state_initial     = env.reset(robot.robot_position_x, robot.robot_position_y)
        state, _          = robot.state
        state             = np.asarray(state)
        score             = 0
        cont              = 0
        robot.diff_time   = 0.20
        # print("START",robot.process)
        # robot.stand=True
        for t in range(agent.episode_step):
            robot.step = t
            action,evolve_rule = agent.get_action(state)
            print(evolve_rule,robot.process)
            # print(np.rad2deg(robot.heading
            if agent.Pa >agent.start_or:
                if robot.process =="collision":
                    break
                else:
                    pass

            if evolve_rule or robot.process =="collision":
                action = robot.evolve()
                robot.perform_action(action)
                next_state, reward, done = robot.next_values(action)
                # print("evolve step", action)

            else:
                robot.perform_action(action)
                print("action nn: ", action)
                # print(robot.cmd_vel)
                next_state, reward, done = robot.next_values(action)
            # print("1",robot.process)
            agent.tf_rewards=reward
            #print(reward)
            next_state[0:24]=next_state[0:24]/3.5
            # next_state[24]=(next_state[24]+np.pi)/2*np.pi
            next_state[25]= next_state[25]/env._goal_distance_initial
            next_state[26]=next_state[26]/3.5
            next_state[27]=next_state[27]/24.0           

            agent.append_D(state, action, reward, next_state, done)
            agent.append_EPS(state, action, reward, next_state, done)
            if not os.path.exists(agent.dirPath+"_map"+'.txt'):
                with open(agent.dirPath+"_map"+'.txt','a') as outfile:
                    outfile.write("state".rjust(150," ")+"   "+"robot_x".rjust(10," ")\
                    +"   "+"robot_y".rjust(10," ")+"   "+"target_x".rjust(10," ")\
                    +"   "+"target_y".rjust(10," "))
            with open(agent.dirPath +"_map"+ '.txt', 'a') as outfile:
                outfile.write(' '.join(np.array(state).astype(str))+"   "\
                +"{: .3e}".format(robot.robot_position_x)+"   "\
                +"{: .3e}".format(robot.robot_position_y)+"   "\
                +"{: .3e}".format(env.goal_x)+"   "+"{: .3e}".format(env.goal_y) +"\n")

            if not os.path.exists(agent.dirPath +"_value"+'.txt'):
                with open(agent.dirPath +"_value"+'.txt', 'a') as outfile:
                    outfile.write("step".rjust(8," ")+ "   "+"episode".rjust(8," ")\
                    + "   "+"a".rjust(1," ")+"   "+"   "+"reward".rjust(10," ")\
                    +"   "+"score".rjust(10," ")+"   "+"robot_x".rjust(10," ")\
                    +"   "+"robot_y".rjust(10," ")+"  "+"goal_x".rjust(10," ")\
                    +"   "+"goal_y".rjust(10," ") +"   " +"e_r".rjust(1," ")\
                    +"   "+"q_value".rjust(8," ")+"   "+"time".rjust(10," ")\
                    +"   " +"win".rjust(4," ")+"   " +"fail".rjust(4," ")\
                    +"   " +"ep".rjust(10," ")\
                    +"   "+"t_h".rjust(2," ")+"   "+"t_m".rjust(2," ")\
                    +"   "+"t_s".rjust(2," ")+"\n")

            m, s = divmod(int(time.time() - start_time), 60)
            h, m = divmod(m, 60)
            with open(agent.dirPath +"_value"+'.txt', 'a') as outfile:
                outfile.write("{:8d}".format(t)+"   "+"{:8d}".format(ep)\
                +"   "+str(action)+"   "+"   "+"{: .3e}".format(reward)\
                +"   "+"{: .3e}".format(score)+"   "+"{: .3e}".format(robot.robot_position_x)\
                +"   "+"{: .3e}".format(robot.robot_position_y)+"   "+"{: .3e}".format(env.goal_x) \
                +"   "+"{: .3e}".format(env.goal_y) +"   " +str(int(evolve_rule))\
                +"   "+"{: .3e}".format(np.max(agent.q_value))+"   "+"{: .3e}".format(env.best_time)\
                +"   " +str(int(env.get_goalbox))+"   " +str(int(done))\
                +"   "+"{: .3e}".format(agent.Pa)\
                +"   "+"{:8d}".format(h)+"   "+"{:02d}".format(m) \
                +"   "+"{:02d}".format(s) +"\n")

            if  env.get_goalbox ==True:
                if env.best_time > 0.85:
                    agent.best_state()
                else:
                    agent.winning_state()

            if  env.get_goalbox ==True or done==True:
                m, s = divmod(int(time.time() - start_time), 60)
                h, m = divmod(m, 60)
                if not os.path.exists(agent.dirPath +"_time_goal"+'.txt'):
                    with open(agent.dirPath +"_time_goal"+'.txt', 'a') as outfile:
                        outfile.write("episode".rjust(8," ")+ "   "+"   "+"m".rjust(10," ")\
                        +"   "+"robot_x".rjust(10," ")\
                        +"   "+"robot_y".rjust(10," ")+"  "+"goal_x".rjust(10," ")\
                        +"   "+"goal_y".rjust(10," ") \
                        +"   "+"g".rjust(4," ")\
                        +"   " +"f".rjust(4," ")\
                        +"   "+"t_h".rjust(4," ")+"   "+"t_m".rjust(4," ")\
                        +"   "+"t_s".rjust(4," ")+"   " +"ep".rjust(4," ")+"\n")
                with open(agent.dirPath +"_time_goal"+'.txt', 'a') as outfile:
                    outfile.write("{:8d}".format(ep)+"   "+"{:8d}".format(len(agent.memory_D))\
                    +"   "+"{: .3e}".format(robot.robot_position_x)\
                    +"   "+"{: .3e}".format(robot.robot_position_y)+"   "+"{: .3e}".format(env.goal_x) \
                    +"   "+"{: .3e}".format(env.goal_y) \
                    +"   " +str(int(env.get_goalbox))+"   " +str(int(done))\
                    +"   "+"{:8d}".format(h)+"   "+"{:02d}".format(m) \
                    +"   "+"{:02d}".format(s)\
                    +"   "+"{: .3e}".format(agent.Pa)+"\n")

                if agent.Pa <agent.start_or:
                    # Calculate the distance to the target from the agent's last position
                    env.get_goalbox = False
                    env.goal_x, env.goal_y = env.target_position.getPosition(True, delete=True)
                    robot.last_heading=list()
                    # print(robot.last_heading)
                    env.get_Distance_goal(robot.robot_position_x,robot.robot_position_y)
                    # print("initial: ",robot.heading)

                else:
                    # Returns to the agent's origin position and calculates the distance to the target
                    env.get_goalbox = False
                    robot.reset()
                    env.reset_gazebo()
                    robot.last_heading=list()

                    env.goal_x, env.goal_y = env.target_position.getPosition(True, delete=True)
                    env.get_Distance_goal(robot.robot_position_x,robot.robot_position_y)
                # robot.stand=True
            # print("2",robot.process)

            if  len(agent.memory_D) > (agent.train_start):
            # if (agent.Pa >=agent.lim_train) and (len(agent.memory_D) > 1*(agent.train_start)):
            # if agent.Pa >=agent.lim_q_i-0.006:
                # print(agent.Pa)
                #agent.train_model()
                print("training")
            score += reward
            state = next_state
            if e % 5 == 0:
                agent.q_model.save(agent.dirPath + str(e)+'_q_model' +'.h5')
                agent.target_model.save(agent.dirPath + str(e) +'_target_model'+'.h5')
                param_keys = ['Pa','Pbest']
                param_values = [agent.Pa, agent.Pbest]
                param_dictionary = dict(zip(param_keys, param_values))
                with open(agent.dirPath + str(e) + '.json', 'w') as outfile:
                    json.dump(param_dictionary, outfile)
            # print("t" ,t)
            if t >= 500:
                rospy.loginfo("Time out!!")
                # robot.stand=True
                finish=True
                done = True
            # print("3",robot.process)

            if done:
                agent.update_target_network()

                if finish:
                    finish = False
                    break

                if evolve_rule:
                    # print("I am a popo")
                    robot.process="collision"
                    ep +=1
                    cont+=1

                    if agent.Pbest < agent.Pbest_max:
                        agent.Pbest /= agent.increase_factor
                    elif agent.Pbest > agent.Pbest_max:
                        agent.Pbest = agent.Pbest_max
                        agent.Pa = agent.Pbest_max
                    else:
                        agent.Pbest = agent.Pbest
                    agent.get_Pa()
                    robot.last_heading=list()

                    # print(robot.last_heading)
                    env.reset(robot.robot_position_x,robot.robot_position_y)
                    # print("initial: ",robot.heading)

                    if cont > 20:
                        cont=0
                        break
                else:
                    break
            # print("4",robot.process)


            global_step += 1
            if global_step % agent.target_update == 0:
                agent.update_target_network()

        if agent.Pbest < agent.Pbest_max:
            agent.Pbest /= agent.increase_factor

        elif agent.Pbest > agent.Pbest_max:
            agent.Pbest = agent.Pbest_max
            agent.Pa = agent.Pbest_max
        else:
            agent.Pbest = agent.Pbest
        agent.get_Pa()
